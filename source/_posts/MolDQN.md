---
title: Moldqn论文阅读笔记
date: 2021-04-28 13:20:21
tags: ["机器学习","强化学习","DQN"]
categories: "学习"
mathjax: true
---

moldqn所说的与之前模型的不同，大多都针对gcpn

- 基于价值函数，低方差、稳定、采样效率高

- 不需要专家数据，从自己的经验学习

- 多目标强化学习，用户自己定权重

<!-- more -->

动作：原子添加、键添加、键移除

## MDP

1. 一个显式步数限制（与GCPN相同）

2. 违反价约束的动作不会被考虑（GCPN中会给这些动作一个惩罚）

3. 允许键、边的移除（GCPN中只能添加）

### 状态空间

（m, t) 

1. m是一个有效分子，t是从起始状态到目前分子的步数

2. 起始状态中，m可以是一个特定分子或什么都不是，t起始为0

3. 限制最大步数为T，那么最终的状态为$\{s=(m,t)|t=T\}$, 也就是那些达到T步的分子，所有轨迹步长相同（GCPN会传入停止符导致步长不同）

### 动作空间

1. 原子添加：原子添加总是伴随着键添加，加入一个新原子并在已有分子与这个原子之间添加一个化学键。

2. 化学键添加：遵守以下规则
- No bond $\rightarrow${Single, Double, Triple} Bond.
- Single bond $\rightarrow${Double, Triple} Bond.
- Double bond $\rightarrow${Triple} Bond.

  同时：环内不允许键的添加，只允许生成拥有3-6个原子的环。但还是会出现多于6个原子的环结构，只要先生成一个双环结构，再去掉它的桥接键。

3. 化学键去除：遵守以下规则
- Triple bond $\rightarrow${Double, Single, No} Bond.

- Double bond $\rightarrow${Single, No} Bond.

- Single bond $\rightarrow${No} Bond.

  同时：除了同时去掉一个原子的情况，不允许在去掉化学键后生成两个不连接的图，也就是说去掉键后还要是一个分子。
  
  在设计分子时不会主动破坏芳香键，但去除外环双键也会破坏芳香性。芳香性也可以通过逐步生成单双键形成。
  
4. 转移概率：确定策略
5. 折扣因子：奖励影响从最终到起始逐渐衰退，在奖励设定时隐式设定，在得到中间产物时也判定性质奖励。

## 多目标强化学习

多个奖励取权重和

## 训练中的探索与开发

策略网络的探索与开发都是从网络获得的动作分布中采样，本身就存在一定的探索能力，gcpn通过对熵的减小进行惩罚进一步保证了模型的探索能力。

价值函数则可以直接利用得到的状态价值找到最优的动作，但一位地采取最优动作会使模型丧失探索能力，因此有各种提高探索能力的方法。

dithering strategy：例如$\epsilon$贪婪 不考虑短期探索，只有一步随机（抖动），会导致数据需求指数级增长（数据也就是智能体生成采样，但抖动策略都很短视，想要拿到长期回报高的就需要大量数据，当然所需时间也会指数级增长）

但抖动策略完全没有考虑到状态的不确定性，只是以一定概率采用随机动作，没法进行深度的探索。

molDQN采用bootstrapped-dqn来解决这个问题，它有H个独立的价值函数$$\{Q^{(i)}|i=1,...,H\}$$,每一个值函数都用样本的不同子集进行训练，在每个episode，都规范化地选择一个值函数$$Q^{(i)}$$来执行决策。通过这些值函数的不同初始化，来使得模型具有一定的深度探索能力。

